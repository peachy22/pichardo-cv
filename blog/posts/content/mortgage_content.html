<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcjmcg8hejn3hw0tp6jJPlDgVsDvb5NEtz-DlIjd4YwrlIklkKP2-BKDngZQ4MY72VRU5O_KkRM4WCslEvZ8C-B_hSza0jGiaZivF4iGP5ETUABxMrt_iicm5-jSQk3ye2Qgol9FV6yIG-mbpxsmYF47t1hDyWapW6QGYWx11gy9IF3cMYAQaHm7Tx/s1332/photo-1524813686514-a57563d77965.webp" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="749" data-original-width="1332" height="360" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhcjmcg8hejn3hw0tp6jJPlDgVsDvb5NEtz-DlIjd4YwrlIklkKP2-BKDngZQ4MY72VRU5O_KkRM4WCslEvZ8C-B_hSza0jGiaZivF4iGP5ETUABxMrt_iicm5-jSQk3ye2Qgol9FV6yIG-mbpxsmYF47t1hDyWapW6QGYWx11gy9IF3cMYAQaHm7Tx/w640-h360/photo-1524813686514-a57563d77965.webp" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><span style="color: #999999;"><i><br />The following was submitted as part of my graduate capstone sequence, and as such had to conform to very specific guidelines for language use, method justification, and overall presentation.<span><!--more--></span><span></span></i></span></td></tr></tbody></table><h2 style="text-align: left;">Research Question</h2><div>“Can a random forest model trained on mortgage application outcomes in the state of Delaware be trained on HMDA data to predict the outcome applications unseen by the model?”</div><div><br /></div><div>It is typical that a loan applicant with very low income and a high-cost loan request will be rejected; it is unlikely that the bank will be paid back in such cases. There are other factors as well that factor in to loan decisions. This data study contributes research towards the prediction of home-loan approval using these and similary predictors. It will be of interest to prospective applicants, finance professionals, and the MSDA program. The government requires mortgage lending institutions to make comprehensive data on home loans (that nonetheless protects the privacy of borrowers) available for public consideration- the stated purpose is so financial institutions may have their practices scrutinized (Office of Law Revision Council, 1975). The data is available in a year-by-year format from the Federal Financial Institutions Examination Council and represents millions of applications, demographic and real-estate features for each, as well as a flag indicating the decision of the lending institution. A supervised learning algorithm will be applied to the classification of applications into the categories of ‘originated’ (accepted) or not (rejected). The study is limited to the state of Delaware and the data available for 2020. Supervised learning is appropriate because we wish to learn patterns on labeled historical data and then predict on new data with the same structure (Nasteski, 2017). The programming language chosen for this project is Python, and the supervised algorithm is random forest through the sklearn.ensemble module. Tumuleru et al. published a paper on random forest classification for several typical metrics such as income, age, debt-income ratio, and more towards the target outcome of ‘loan approved’ or ‘loan rejected’; after hypothesizing the efficacy of several different types of ML algorithms, they concluded that random forests achieved the desired accuracy (Tumuleru, 2022). This being the case, yet not specifically for mortgage lending, suggests a need for the study outlined in this paper.</div><div><br /></div><div>The hypotheses under consideration are presented, which are tested by application of McNemar’s Test, a form of chi-square which verifies if a classification algorithm performs better than the naive baseline based upon the proportion of the dominant class. The McNemar test was used in the (Cansiz, 2021) study on binary classification.</div><div><br /></div><div>H0: A random forest model that attains greater predictive accuracy, as measured by a McNemar test vs that of a naive model, on unseen mortgage application outcomes can be trained on the available data.</div><div><br /></div><div>H1: A random forest model that attains greater predictive accuracy, as measured by a McNemar test vs that of a naive model, on unseen mortgage application outcomes cannot be trained on the available data.</div><h2 style="text-align: left;">Data Collection</h2><div>The dataset is requisitioned in .csv format from Modified Loan/Application Register, Snapshot National Loan Level Dataset (FFIEC, 2020). An advantage of the way this set is made available is that data is available for separate years starting 2017, and a link to the variable schema is available from the same page. A major problem is that there is no way to filter the download by state, so as a result the file is over 10GB in size when uncompressed. A file of this size will immediately use up available memory, though only a small part of it (Delaware state rows) is required. This is overcome by utilizing code to read the .csv file one small chunk at a time and storing only those chunks with the ‘DE’ flag in the state variable.</div><h2 style="text-align: left;">Data Extraction and Preparation</h2><div>The python modules used in this analysis are as follows:</div><div><br /></div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><span style="font-family: courier;">import pandas as pd #dataframe object type</span></div><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><span style="font-family: courier;">import seaborn as sns #attractive plotting</span></div><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><span style="font-family: courier;">import matplotlib.pyplot as plt #additional plotting tools</span></div><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><span style="font-family: courier;">import numpy as np #scientific computing across dataframes</span></div><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><span style="font-family: courier;">from sklearn.model_selection import train_test_split #automatic split of training and testing data</span></div><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><span style="font-family: courier;">from sklearn.ensemble import RandomForestRegressor #key supervised learning algorithm</span></div><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><span style="font-family: courier;">from sklearn.tree import plot_tree #decision tree visualization</span></div><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><span style="font-family: courier;">from statsmodels.stats.contingency_tables import mcnemar #hypothesis test for model significance</span></div><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><span style="font-family: courier;">import warnings #hide warnings for certain code blocks</span></div><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><span style="font-family: courier;">import csv #write reproducible results</span></div></pre></div><div><br /></div><div>These combined with base Python 3 allow for data requisition and cleaning according to the needs of the analysis. This process begins with the extraction of data from the locally-stored large file. The code below reads the file in manageable chunks and stores the observations flagged for the state of Delaware. This code generates as many warnings as there are chunks for columns having mixed data types. The filterwarnings call prevents this output.</div><div><br /></div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div>warnings.filterwarnings('ignore')</div><div>#instantiate empty list</div><div>chunk_list = []</div><div>#read raw data in chunks and store delaware rows in list</div><div>for chunk in pd.read_csv('/Volumes/EXT128/WGU/Capstone/2020_public_lar_one_year.csv', chunksize=500000):</div><div>&nbsp; &nbsp; chunk_list.append(chunk[(chunk['state_code'] == 'DE')])</div></div></pre></div><div><br /></div><div>The observations of chunk_list are concatenated into the key dataframe of the analysis, entitled de for Delaware.</div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><span style="font-family: courier;">#pandas dataframe for desired subset</span></div><div><span style="font-family: courier;">de = pd.concat(chunk_list)</span></div><div><span style="font-family: courier;">#display sample</span></div><div><span style="font-family: courier;">pd.options.display.max_columns = None</span></div><div><span style="font-family: courier;">de.head()</span></div></div></div></pre></div></div><div><br /></div><div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqIwdR3JO2mzAWvPe-12EeXqNB9W1YG0r4DNkljJsUW_5YHgegLopE8pUf-hV8teP5wa4WyeJ26z8s_-_lSkBDOg64TouWCH4DP1Z5uPzf8ieUHYKt86VN4vNNvIo8ZxLCmmpKY-_60bEZ_LFITQLBCDb1u2F8m591NDHGqAP4GNvfFZhCn3p9cRkx/s1184/Screen%20Shot%202022-11-04%20at%201.07.37%20PM.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="232" data-original-width="1184" height="126" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqIwdR3JO2mzAWvPe-12EeXqNB9W1YG0r4DNkljJsUW_5YHgegLopE8pUf-hV8teP5wa4WyeJ26z8s_-_lSkBDOg64TouWCH4DP1Z5uPzf8ieUHYKt86VN4vNNvIo8ZxLCmmpKY-_60bEZ_LFITQLBCDb1u2F8m591NDHGqAP4GNvfFZhCn3p9cRkx/w640-h126/Screen%20Shot%202022-11-04%20at%201.07.37%20PM.png" width="640" /></a></div><div><br /></div>Some of the dataset’s nature is visible in the head call. Evident is the large number of categorical variables, some continuous variables and also several columns that appear to be largely NaN. The sparsity of the set is investigated:</div><div><br /></div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><span style="font-family: courier;">#compute and print sparsity</span></div><div><span style="font-family: courier;">spar = de.isnull().sum().sum()/(len(de.axes[0])*len(de.axes[1]))</span></div><div><span style="font-family: courier;">print('Data Sparsity: ', spar)</span></div></div></div></pre></div></div><div><br /></div><div><span style="color: #990000; font-family: courier;">Data Sparsity: 0.31783593423409684</span></div><div><br /></div><div>This initial data is very sparse at 32%. A goal of the data preparation is to reduce this to zero. Prior to this is a brief introduction to the available values for the target variable, using a barchart.</div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#display outcome distribution</span></div><div><span style="font-family: courier;">outcome = sns.countplot(x=de["action_taken"])</span></div><div><span style="font-family: courier;">outcome.set(xlabel ="Action Taken", ylabel = "Count", title ='Distribution of Outcomes')</span></div></div></div></div></pre></div></div><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdruncbKQ9usVNDKVF0fEbQuyLOiUyK-Dd7_0TJV2NJI6KsDJyBJBxS8EodFQ_vmOPJQfSXFtKiDyFD4ZjnCtg5L52zCrTjQIEqTpP7uX5g-JtqJYzkkU6IT43_o7_775d8ROICeWNXH4YorCDfrQUN44y8hlderiraBdwGJTYEfo7RxH-mR8etHrs/s587/Screen%20Shot%202022-11-04%20at%201.07.53%20PM.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="452" data-original-width="587" height="492" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdruncbKQ9usVNDKVF0fEbQuyLOiUyK-Dd7_0TJV2NJI6KsDJyBJBxS8EodFQ_vmOPJQfSXFtKiDyFD4ZjnCtg5L52zCrTjQIEqTpP7uX5g-JtqJYzkkU6IT43_o7_775d8ROICeWNXH4YorCDfrQUN44y8hlderiraBdwGJTYEfo7RxH-mR8etHrs/w640-h492/Screen%20Shot%202022-11-04%20at%201.07.53%20PM.png" width="640" /></a></div><div>Critical to this project is an understanding of the various codes available for loan decisions. The FFIEC schema for this variable is as follows, presented for clarity (FFIEC, 2019).</div><div><br /></div><div><i><span>&nbsp;&nbsp; &nbsp;</span>1 - Loan Originated</i></div><div><i>Use this code for an application that was originated, including an originated loan that resulted from a preapproval request.</i></div><div><i><span>&nbsp;&nbsp; &nbsp;</span>2 - Application Approved But Not Accepted</i></div><div><i>Use this code if the application was approved and a credit decision, but the applicant did not accept (close the loan), or rescinded after closing.</i></div><div><i><span>&nbsp;&nbsp; &nbsp;</span>3 - Application Denied</i></div><div><i>&nbsp;<span>&nbsp;&nbsp; &nbsp;</span> • Use this code if the application was denied.</i></div><div><i>&nbsp;<span>&nbsp;&nbsp; &nbsp;</span> • Use this code if a counteroffer was not accepted.</i></div><div><i><span>&nbsp;&nbsp; &nbsp;</span>4 - Application Withdrawn by Applicant</i></div><div><i><span>&nbsp;&nbsp; &nbsp;</span>&nbsp; • Use this code only when the application is expressly withdrawn by the applicant before a credit decision is made and before the file is closed for incompleteness.</i></div><div><i><span>&nbsp;&nbsp; &nbsp;</span>&nbsp; • Use this code if a conditional approval includes creditworthiness conditions &amp; the applicant expressly withdraws before satisfying.”</i></div><div><i><span>&nbsp;&nbsp; &nbsp;</span>5 - File Closed for Incompleteness</i></div><div><i>Use this code if the applicant was sent a written notice of incompleteness (NOI) and did not respond to the request for more information.</i></div><div><i><span>&nbsp;&nbsp; &nbsp;</span>6 - Loan (re)purchased</i></div><div><i>Repurchased a loan that was previously sold.</i></div><div><i><span>&nbsp;&nbsp; &nbsp;</span>7 - Preapproval request denied</i></div><div><i>Use this code if a Preapproval was denied.</i></div><div><i><span>&nbsp;&nbsp; &nbsp;</span>8 - Preapproval request approved but not accepted</i></div><div><i>Use this code if the Preapproval was approved but the applicant did not accept or borrower indicates they no longer are interested.</i></div><div><br /></div><div>There are a number of outcomes for which it is hard to say if they should be classed as rejected outright. This analysis will be reduced to a binary classification problem by removing the observations that do not fit the originated/rejected paradigm. Before that, the minority nature of rejected loans bear out investigation as many variables may be correlated with rejection exclusively. As said earlier, the sparsity is high. One strategy for reducing NaN content is to delete columns that are largely empty, then run code to drop observations with NAs. If one is not careful with this method, then columns that are NaN because of their association with the minority class will cause all or most minority observations to be deleted. This strategy is employed by first investigating the prevalence of null values within the class of rejected loans.</div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#show structure of rejected loans class</span></div><div><span style="font-family: courier;">de.loc[(de['action_taken'] == 3)].info()</span></div></div></div></div></pre></div></div><div><i><br /></i></div><div><i>[output of 99 variable descriptions hidden for presentation purposes- please run the code if you would like to see this]</i></div><div><br /></div><div>This is a large number of variables, many of which should be removed for model quality. The following code drops undesired columns based on index number. The strategy is to reduce sparsity and correlation between variables by deleting those with information that can be found elsewhere. For instance, extra columns relating to ethnicity and race are mostly left empty. Thus the solution chosen is to remove them in favor of the primary declaration. Another variable, derived_loan_product_type, is defined in the schema as a combination of variables involving construction and property type.</div><div><br /></div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><span style="font-family: courier;">#drop undesired columns&nbsp;</span></div><div><span style="font-family: courier;">de_r = de.drop(de.columns[[0,1,2,3,4,5,6,9,10,11,15,16,17,22,23,24,26,27,28,29,30,32,33,39,43,44,46,50,51,52,53,55,56,57,58,59,60,62,63,64,65,67,68,69,70,71,72,75,76,79,80,81,84,85,86,87,88,89,90,91]],axis=1) &nbsp;&nbsp;</span></div></div></div></pre></div><div>&nbsp; &nbsp; &nbsp;&nbsp;</div><div>Prior to display of data that is kept for analysis, the null values are dropped. Then the datatypes of each are reinforced as either pandas categories or floating-point numbers. This is important because most of the categorical variables are coded as integers per the schema’s loan-industry definitions. Additionaly, the ‘exempt’ class observations of the loan_term variable are dropped because of a low observation count combined with the non-numeric value conflicting with the numeric nature of the variable. Finally, observations with the target variable outcomes 1 and 3 are kept, recoded as 0 and 1 for ‘rejected’ and ‘originated’.</div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#drop observations with null values from reduced frame</span></div><div><span style="font-family: courier;">de_r = de_r.dropna()</span></div><div><span style="font-family: courier;">#drop observations with loan term = 'exempt'</span></div><div><span style="font-family: courier;">de_r.drop(de_r[de_r['loan_term']=='Exempt'].index,inplace=True)</span></div><div><span style="font-family: courier;">#exclude targets outside binary outcome of interest</span></div><div><span style="font-family: courier;">de_r = de_r.loc[(de_r['action_taken'] == 1) | (de_r['action_taken'] == 3)]</span></div><div><span style="font-family: courier;">#recode binary outcome&nbsp;</span></div><div><span style="font-family: courier;">de_r['action_taken'] = np.where(de_r['action_taken'] == 3, 0, 1)</span></div><div><span style="font-family: courier;">#establish data types as categorical or numeric.</span></div><div><span style="font-family: courier;">de_r[['hoepa_status','derived_loan_product_type','derived_dwelling_category','action_taken','purchaser_type','preapproval','reverse_mortgage','open_end_line_of_credit','business_or_commercial_purpose','negative_amortization','interest_only_payment','balloon_payment','other_nonamortizing_features','occupancy_type','manufactured_home_secured_property_type','manufactured_home_land_property_interest','applicant_credit_score_type','co_applicant_credit_score_type','applicant_ethnicity_1','co_applicant_ethnicity_1','applicant_race_1','co_applicant_race_1','applicant_sex','co_applicant_sex','applicant_age','co_applicant_age','initially_payable_to_institution','aus_1']] = de_r[['hoepa_status','derived_loan_product_type','derived_dwelling_category','action_taken','purchaser_type','preapproval','reverse_mortgage','open_end_line_of_credit','business_or_commercial_purpose','negative_amortization','interest_only_payment','balloon_payment','other_nonamortizing_features','occupancy_type','manufactured_home_secured_property_type','manufactured_home_land_property_interest','applicant_credit_score_type','co_applicant_credit_score_type','applicant_ethnicity_1','co_applicant_ethnicity_1','applicant_race_1','co_applicant_race_1','applicant_sex','co_applicant_sex','applicant_age','co_applicant_age','initially_payable_to_institution','aus_1']].astype('category')</span></div><div><span style="font-family: courier;">de_r[['loan_amount','loan_term','property_value','income','tract_population','tract_minority_population_percent','ffiec_msa_md_median_family_income','tract_to_msa_income_percentage','tract_owner_occupied_units','tract_one_to_four_family_homes','tract_median_age_of_housing_units']] = de_r[['loan_amount','loan_term','property_value','income','tract_population','tract_minority_population_percent','ffiec_msa_md_median_family_income','tract_to_msa_income_percentage','tract_owner_occupied_units','tract_one_to_four_family_homes','tract_median_age_of_housing_units']].astype(float)</span></div><div><span style="font-family: courier;">#display info</span></div><div><span style="font-family: courier;">de_r.info()</span></div></div></div></div></pre></div></div><div>&nbsp;<div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMP6Y-5Ia6fZrot2cSxh9l_LY3rulrNQOcURb8FH51KOIQS1SpIAR80OCDImKRUW6i7Y1wTB58MqCsBVUp3E_fsMPX1nXVJ-70qoJjUK6EfeUN4i0y4_aOv6mJ7XaHOubSdNW-INY5CAWw56fTfrrofowkV801AQhEZiCovxEHXru09hf_6wlua6yr/s510/Screen%20Shot%202022-11-17%20at%2010.50.24%20AM.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" data-original-height="510" data-original-width="409" height="480" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMP6Y-5Ia6fZrot2cSxh9l_LY3rulrNQOcURb8FH51KOIQS1SpIAR80OCDImKRUW6i7Y1wTB58MqCsBVUp3E_fsMPX1nXVJ-70qoJjUK6EfeUN4i0y4_aOv6mJ7XaHOubSdNW-INY5CAWw56fTfrrofowkV801AQhEZiCovxEHXru09hf_6wlua6yr/w385-h480/Screen%20Shot%202022-11-17%20at%2010.50.24%20AM.png" width="385" /></a></div><div class="separator" style="clear: both; text-align: left;"><br /></div></div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><span style="font-family: courier;">#display binary outcome distribution</span></div><div><span style="font-family: courier;">binary_outcome = sns.countplot(x=de_r["action_taken"])</span></div><div><span style="font-family: courier;">binary_outcome.set(xlabel ="Action Taken", ylabel = "Count", title ='Rejected or Originated')</span></div></div></div></pre></div><div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDGWNszneU-P9hJhVyVz69w6Wsg9_9g8QG81tS833ojDzmaZgCMlXgp9_y6W9uYmBtg9B_KTdYt6h08OhP2_duRx5BmwkbgELovo68eDJwMXOngmO1qHRenw8YURh6fhqaR69pJfFmE-6BifLHOEpIh-q-v2cbDlfQ4VvcKiX_zmyygsXJD-z7qpLC/s588/Screen%20Shot%202022-11-04%20at%201.08.07%20PM.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="445" data-original-width="588" height="484" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDGWNszneU-P9hJhVyVz69w6Wsg9_9g8QG81tS833ojDzmaZgCMlXgp9_y6W9uYmBtg9B_KTdYt6h08OhP2_duRx5BmwkbgELovo68eDJwMXOngmO1qHRenw8YURh6fhqaR69pJfFmE-6BifLHOEpIh-q-v2cbDlfQ4VvcKiX_zmyygsXJD-z7qpLC/w640-h484/Screen%20Shot%202022-11-04%20at%201.08.07%20PM.png" width="640" /></a></div></div></div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><span style="font-family: courier;">#compute and print reduced frame sparsity</span></div><div><span style="font-family: courier;">spar_r = de_r.isnull().sum().sum()/(len(de.axes[0])*len(de.axes[1]))</span></div><div><span style="font-family: courier;">print('Clean Data Sparsity: ', spar_r)</span></div></div></div></pre></div><div><br /></div><div><span style="color: #990000; font-family: courier;">Clean Data Sparsity: 0.0</span></div><div><br /></div><div>The data has been reduced to 51,950 observations with 0% sparsity, as well as 1 binary target variable and 38 explanatory variables of both a categorical and numeric nature. However, data preparation is not yet done as the random forest regressor cannot utilize categorical variables that are not easily convertable to numeric format. Indeed, many of the explanatory variables come encoded as integers but are generally not ordinal- that is, we cannot assume that the size of an integer that stands for a certain class means it is similar to nearby classes. One method of dealing with this is one-hot encoding the categorical variables. This is performed below utilizing a built-in pandas method. Note that the drop_first argument, which deletes the first column of a one-hot variable to avoid multi-colinearity in classification models, is set to True.</div><div><div>&nbsp;&nbsp; &nbsp;&nbsp;</div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#one-hot encode variables</span></div><div><span style="font-family: courier;">de_r = pd.get_dummies(de_r, drop_first = True, columns = ['hoepa_status','derived_loan_product_type','derived_dwelling_category','purchaser_type','preapproval','reverse_mortgage','open_end_line_of_credit','business_or_commercial_purpose','negative_amortization','interest_only_payment','balloon_payment','other_nonamortizing_features','occupancy_type','manufactured_home_secured_property_type','manufactured_home_land_property_interest','applicant_credit_score_type','co_applicant_credit_score_type','applicant_ethnicity_1','co_applicant_ethnicity_1','applicant_race_1','co_applicant_race_1','applicant_sex','co_applicant_sex','applicant_age','co_applicant_age','initially_payable_to_institution','aus_1'])</span></div></div></div></div></pre></div></div><div><br /></div><div>At this point the data are prepared and ready to be split into training and testing sets. Below the testing proportion is set to 15%, at which point data preparation is considered to be complete.</div><div><div>&nbsp; &nbsp; &nbsp;</div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><span style="font-family: courier;">#extract prediction labels</span></div><div><span style="font-family: courier;">labels = np.array(de_r['action_taken'])</span></div><div><span style="font-family: courier;">#extract predictor variables</span></div><div><span style="font-family: courier;">features= de_r.drop(['action_taken'], axis = 1)</span></div><div><span style="font-family: courier;">#feature names for viariable importance calculation</span></div><div><span style="font-family: courier;">feature_list = list(de_r.drop(['action_taken'], axis = 1).columns)</span></div><div><span style="font-family: courier;">#train/test split from module</span></div><div><span style="font-family: courier;">train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.15, random_state = 43022)</span></div></div></div></pre></div></div><h3 style="text-align: left;">Justification of Preparation Methods</h3><div>In summary, data is acquired and analyzed for null values. Prevalence and location of null values determined the methods for sparsity reduction.</div><div><br /></div><div>The primary strategy for sparsity reduction was employing an understanding of the set, that is, the target outcome of ‘rejected’ was found to be highly correlated with certain fields being null. This made those fields correlated with the target variable in a way that makes them undesired for prediction, so they were removed. The advantage of this strategy is that most of the target observations were retained by the time a function to remove null values was called. A disadvantage was that slight bias in favor of the rejected class was introduced, and many observations that did not fit the binary outcome could not be used.</div><div><br /></div><div>The typical strategy of one-hot encoding categorical variables was applied. This is advantageous because it allows the use of variables in the random forest regressor which are not numeric, and most categorical variables were not ordinal despite being coded as integers. This is disadvantageous in the sense that it removes the ordinal nature of one particular variable which is ordinal. The decision was made to apply this to the applicant_age column, which has an NA value coded as 8888, rather than ordinally recoding because a large number of observations are lost when the error observations are removed.</div><div><br /></div><div>Overall preparation left a class imbalance in favor of originated loans. See model evaluation section for additional info.</div><h2 style="text-align: left;">Analysis</h2><div>A random forest regressor of 25 trees was instantiated with default grid settings and regressed on the training data. Tuning parameters were within memory memory constraints and unaltered after discovery of favorable evaluation metrics on the holdout data set.</div><div><div><div>&nbsp;</div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#instantiate random forest regressor with random state for reproduibility</span></div><div><span style="font-family: courier;">n_tree = 25</span></div><div><span style="font-family: courier;">rf = RandomForestRegressor(n_estimators = n_tree, random_state = 43022)</span></div><div><span style="font-family: courier;">#fit on training features and labels</span></div><div><span style="font-family: courier;">rf.fit(train_features, train_labels)</span></div></div></div></div></pre></div></div></div><div><br /></div><div>The hypothesis of this project is tested according to a McNemar Chi-Square test. This test analyzes the performance of the model by comparing its accuracy to that of a naive classifier that predicts based only on the prevalence of the majority class. McNemar’s test is applied to 2x2 contingency tables to find whether row and column marginal frequencies are equal for paired samples (Cansiz, 2021). The code below extracts the predictions and computes a data frame indicating correctness on the test set for both classifiers.</div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#extract predictions of test set</span></div><div><span style="font-family: courier;">predictions_ = rf.predict(test_features)</span></div><div><span style="font-family: courier;">#predictions are probabilities, therefore round and convert to integer type</span></div><div><span style="font-family: courier;">predictions = predictions_.round()</span></div><div><span style="font-family: courier;">predictions = ([int(x) for x in predictions])</span></div><div><span style="font-family: courier;">#vector of correct predictions; 1 = correct, 0 = incorrect</span></div><div><span style="font-family: courier;">pred_correct = np.where(predictions == test_labels, 1, 0)</span></div><div><span style="font-family: courier;">#naive classifer: binomial distribution with p = majority class proportion</span></div><div><span style="font-family: courier;">no_info = np.random.binomial(n=1, p=sum(de_r['action_taken'])/len(de_r), size=[len(pred_correct)])</span></div><div><span style="font-family: courier;">#vector of naive classifier correct predictions</span></div><div><span style="font-family: courier;">naive_correct = np.where(no_info == test_labels, 1, 0)</span></div><div><span style="font-family: courier;">#data frame for comparison</span></div><div><span style="font-family: courier;">compare = pd.DataFrame({'pred_correct':pred_correct,'naive_correct':naive_correct})</span></div><div><span style="font-family: courier;">#view result</span></div><div><span style="font-family: courier;">compare.sample(5)</span></div></div></div></div></pre></div></div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgU8tftBERO4UUjC_jFvp1jjLxjx5cZWH1eZGbQnER6XfizJRGqH75h9bhzehvnQ6WMd6PBUYryGLSc0hMM58fc9Oshe6AfgLbGB03Xc5AopmnHWBOPAiINKmqHUtKHLfxFtZZf9_9wjsG3yIKw_C8UYlLTcKA3gnq0uDoXXtj5zVXdTIAZMcUAqeaG/s244/Screen%20Shot%202022-11-04%20at%201.08.19%20PM.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="165" data-original-width="244" height="135" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgU8tftBERO4UUjC_jFvp1jjLxjx5cZWH1eZGbQnER6XfizJRGqH75h9bhzehvnQ6WMd6PBUYryGLSc0hMM58fc9Oshe6AfgLbGB03Xc5AopmnHWBOPAiINKmqHUtKHLfxFtZZf9_9wjsG3yIKw_C8UYlLTcKA3gnq0uDoXXtj5zVXdTIAZMcUAqeaG/w200-h135/Screen%20Shot%202022-11-04%20at%201.08.19%20PM.png" width="200" /></a></div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#compute overall naive accuracy</span></div><div><span style="font-family: courier;">naive_acc = sum(compare['naive_correct']==1)/len(compare['naive_correct'])</span></div><div><span style="font-family: courier;">#compute overall rf accuracy</span></div><div><span style="font-family: courier;">rf_acc = sum(compare['pred_correct']==1)/len(compare['pred_correct'])</span></div><div><span style="font-family: courier;">#display results</span></div><div><span style="font-family: courier;">print('The naive classifier achieved',round(naive_acc*100),'% accuracy.')</span></div><div><span style="font-family: courier;">print('The random forest classifier achieved',round(rf_acc*100),'% accuracy.')</span></div></div></div></div></pre></div></div><div><br /></div><div><span style="color: #990000; font-family: courier;">The naive classifier achieved 74 % accuracy.</span></div><div><span style="color: #990000; font-family: courier;">The random forest classifier achieved 99 % accuracy.</span></div><div><br /></div><div>The significance of the performance difference between the two classifiers is tested using a contingency table derived from the comparison data frame and the mcnemar function available from statsmodels.</div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#create matrix of accuracy data</span></div><div><span style="font-family: courier;">cc = len(compare[(compare.pred_correct == 1)&amp;(compare.naive_correct == 1)])</span></div><div><span style="font-family: courier;">cw = len(compare[(compare.pred_correct == 1)&amp;(compare.naive_correct == 0)])</span></div><div><span style="font-family: courier;">wc = len(compare[(compare.pred_correct == 0)&amp;(compare.naive_correct == 1)])</span></div><div><span style="font-family: courier;">ww = len(compare[(compare.pred_correct == 0)&amp;(compare.naive_correct == 0)])</span></div><div><span style="font-family: courier;">#define contingency table</span></div><div><span style="font-family: courier;">table = [[cc, wc],</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[cw, ww]]</span></div><div><span style="font-family: courier;">#calculate mcnemar test</span></div><div><span style="font-family: courier;">result = mcnemar(table, exact=True)</span></div><div><span style="font-family: courier;">#summarize the finding</span></div><div><span style="font-family: courier;">print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))</span></div><div><span style="font-family: courier;"># interpret the p-value</span></div><div><span style="font-family: courier;">alpha = 0.05</span></div><div><span style="font-family: courier;">if result.pvalue &gt; alpha:</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; print('Same proportions of errors; fail to reject H0')</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; print('The performance of the naive classifier has not been exceeded.')</span></div><div><span style="font-family: courier;">else:</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; print('Different proportions of errors; reject H0')</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; print('The performance of the naive classifier has likely been exceeded.')</span></div></div></div></div></pre></div></div><div><br /></div><div><span style="color: #990000; font-family: courier;">statistic=55.000, p-value=0.000</span></div><div><span style="color: #990000; font-family: courier;">Different proportions of errors; reject H0</span></div><div><span style="color: #990000; font-family: courier;">The performance of the naive classifier has likely been exceeded.</span></div><div><br /></div><div>The distribution of errors for the random forest classifier is not normally distributed, nor is it expected to be given the binary nature of outcomes. Nonetheless it is provided- a -1 indicates a prediction of 0 where 1 was correct and a 1 indicates a prediction of 0 when 1 was correct.</div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#compute residuals</span></div><div><span style="font-family: courier;">residuals = predictions - test_labels</span></div><div><span style="font-family: courier;">#code visualizatoin of errors</span></div><div><span style="font-family: courier;">sns.histplot(residuals,discrete=True).set(title='Distribution of Binary Classification Residuals')</span></div><div><span style="font-family: courier;">plt.xticks([-1,0,1])</span></div></div></div></div></pre></div></div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWQsmN7rYqqMByGjhKhpJSD2gLbyunzgn_y4awq3pi5q2jbrCN4wN7VPibgr8H0lXtgvnFY9ckEABcPrWIXCp-lx1aco4xnKvvYFLTK_-YVzgg7nBO90_GJvVU0Tg3PwAQKJBVCfe0-Haf0LCa8GGs57eJj0rqxyhglHkLuqmp5bueEg9u6UJUcLIR/s571/Screen%20Shot%202022-11-04%20at%201.08.27%20PM.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="426" data-original-width="571" height="478" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWQsmN7rYqqMByGjhKhpJSD2gLbyunzgn_y4awq3pi5q2jbrCN4wN7VPibgr8H0lXtgvnFY9ckEABcPrWIXCp-lx1aco4xnKvvYFLTK_-YVzgg7nBO90_GJvVU0Tg3PwAQKJBVCfe0-Haf0LCa8GGs57eJj0rqxyhglHkLuqmp5bueEg9u6UJUcLIR/w640-h478/Screen%20Shot%202022-11-04%20at%201.08.27%20PM.png" width="640" /></a></div><div>For aiding in the results summary, the variable importance of the regressor is provided.</div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#extract variable importance</span></div><div><span style="font-family: courier;">importances = list(rf.feature_importances_)</span></div><div><span style="font-family: courier;">#assign names</span></div><div><span style="font-family: courier;">feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]</span></div><div><span style="font-family: courier;">#sort list of feature importances</span></div><div><span style="font-family: courier;">feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)</span></div><div><span style="font-family: courier;">#report top 5 features</span></div><div><span style="font-family: courier;">[print('Feature: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:5]]</span></div></div></div></div></pre></div></div><div><br /></div><div><span style="color: #990000; font-family: courier;">Feature: hoepa_status_3 Importance: 0.56</span></div><div><span style="color: #990000; font-family: courier;">Feature: occupancy_type_2 Importance: 0.22</span></div><div><span style="color: #990000; font-family: courier;">Feature: occupancy_type_3 Importance: 0.07</span></div><div><span style="color: #990000; font-family: courier;">Feature: income Importance: 0.02</span></div><div><span style="color: #990000; font-family: courier;">Feature: purchaser_type_1 Importance: 0.02</span></div><h3 style="text-align: left;">Justification of Analysis Methods</h3><div>Random Forest is advantageous for the mix of categorical and numeric data present in the set. A similar study was performed on loan data (not mortgages) where random forest acheived the desired accuracy over other classification algorithms (Tumuleru, 2022) and this fact informed the selection of an algorithm. Some studies have cited a disadvantage using random forests with one-hot encoded variables- specifically that it induces data sparsity and can make feature importance difficult to interpret (Ravi, 2019).</div><h2 style="text-align: left;">Data Summary and Implications</h2><div>In summary, random forest was used to regress the predictor variables towards classification of loan applications into ‘originated’ or ‘rejected’. The finding was significant, 99% accuracy which is better than a naive classifier based upon the prevalence of the majority ‘originated’ class. The null hypothesis that a very effective prediction algorithm cannot be developed is rejected.</div><div><br /></div><div>The target classes were imbalanced in favor of originated loans. This could lead to misleading accuracy metrics, however the high accuracy combined with McNemar’s test mitigated the problem by considering the accuracy inherent in predicting based upon the likelihood of the majority class. The limitation of easily interpreting the variable significance, a result of the one-hot encoding discussed at the end of the previous section, stands. Below we visualize one of the decision trees used in the random forest regressor. Only the first few layers are displayed and discussed because of the prevalence of only the highly important variables warrant discussion.</div><div><br /></div><div><div><pre class="r" style="background-color: whitesmoke; border-radius: 4px; border: 1px solid rgb(204, 204, 204); box-sizing: border-box; color: #333333; font-size: 13px; line-height: 1.428571; margin-bottom: 10px; margin-top: 0px; overflow-wrap: break-word; overflow: auto; padding: 9.5px; word-break: break-all;"><div style="caret-color: rgb(0, 0, 0); color: black; white-space: normal;"><div><div><div><span style="font-family: courier;">#create sample tree figure</span></div><div><span style="font-family: courier;">fig = plt.figure(figsize=(15, 10))</span></div><div><span style="font-family: courier;">plot_tree(rf.estimators_[0],&nbsp;</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; feature_names=feature_list,</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; class_names=['rejected','originated'],&nbsp;</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; filled=True, impurity=True,&nbsp;</span></div><div><span style="font-family: courier;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; rounded=True, max_depth = 2)</span></div><div><span style="font-family: courier;">fig.savefig('tree.png')</span></div></div></div></div></pre></div></div><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjauLV-LjSUgOcL40o9r36VuPd1dwGan_i8UasEnxLYKfvzwJ-BwBEK9MNxURfZ6f2QOgNTfk9_obaBWLpCn2uYtNujfXIjO_aQmmaJygJIM21NagSGPJLNbESEmM8DR8_nxj-njhTGd2QoQxJhtyGPLpCDxCYtLv-8cdNMxRlnqxb1NQXt556BlJqV/s818/Screen%20Shot%202022-11-04%20at%201.08.40%20PM.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="592" data-original-width="818" height="464" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjauLV-LjSUgOcL40o9r36VuPd1dwGan_i8UasEnxLYKfvzwJ-BwBEK9MNxURfZ6f2QOgNTfk9_obaBWLpCn2uYtNujfXIjO_aQmmaJygJIM21NagSGPJLNbESEmM8DR8_nxj-njhTGd2QoQxJhtyGPLpCDxCYtLv-8cdNMxRlnqxb1NQXt556BlJqV/w640-h464/Screen%20Shot%202022-11-04%20at%201.08.40%20PM.png" width="640" /></a></div><div><br /></div><div>The most significant variable is hoepa_status. We define this according to the schema: “Whether the covered loan is a high-cost mortgage” (FFIEC, 2019). Recalling the data cleaning section, this variable was kept in favor of other loan cost variables due to its high density. The decision tree indicates that a HOEPA status other than ‘3’ was strongly associated with applications that were originated. HOEPA status 3, per the schema, indicates a loan that is exempt from this reporting requirement. Guidelines (National Credit Union, 2014) indicate that HOEPA-exempt mortgages are associated with reverse mortgages, initial constructions, and business-purpose properties. The suggestion of this type of result is that these types of loan applications are more highly scrutinized.</div><div><br /></div><div>The course of action recommended at this time is to operate under the assumption that standard mortgage applications for residential, non-business buyers are very likely to be accepted.</div><div><br /></div><div>I recommend direct efforts towards further pruning the dataset with the goal of developing classifiers that can predict loan acceptance given a loan type. i) Performing a similar analysis with data subset according to the HOEPA flag may lead to more granular results for nonstandard mortgages or for borrowers concerned with residential purchases. ii) Modeling this data with the inclusion of location variables may give insight to geographic trends in loan acceptance.</div><h2 style="text-align: left;">References</h2><div>Office of Law Revision Council. (Dec 31, 1975). Retrieved Oct 11, 2022 from https://uscode.house.gov/view.xhtml?path=/prelim@title12/chapter29&amp;edition=prelim</div><div><br /></div><div>Nasteski, V. (Dec 2017). Retrieved Oct 11, 2022 from https://www.researchgate.net/publication/328146111_An_overview_of_the_supervised_machine_learning_methods</div><div><br /></div><div>Tumuleru, P et al. (Feb 23 2022). Retrieved Oct 12, 2022 from https://ieeexplore.ieee.org/document/9742800</div><div><br /></div><div>Cansiz, S. (Mar 7 2021). Retrieved Oct 13, 2022 from https://towardsdatascience.com/have-you-ever-evaluated-your-model-in-this-way-a6a599a2f89c</div><div><br /></div><div>FFIEC. (2020). Retrieved Oct 13, 2022 from https://ffiec.cfpb.gov/data-publication/modified-lar/2021</div><div><br /></div><div>FFIEC. (2019). Retrieved Oct 21, 2022 from https://ffiec.cfpb.gov/documentation/2019/lar-data-fields/</div><div><br /></div><div>Ravi R. (Jan 11, 2019). Retrieved Oct. 24, 2022. from https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769</div><div><br /></div><div>National Credit Union. (Jan 10, 2014). Retrieved Oct. 24, 2019 from https://www.ncua.gov/files/publications/regulation-supervision/RA2013-09-Attachment-NCUA-Dodd-Frank%20Act-HOEPA-Loans-Summary.pdf</div>